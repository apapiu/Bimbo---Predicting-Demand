---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---

**Lemma 1** : Given an array $X = [x_1, x_2,..., x_n]$ the mean of this array is the unique value that minimizes the sum of squares:
$f(x) =  \sum\limits_{i = 1}^n (x_i - x)^2$.

Proof: To find the extrema of $f$ we'll take the first deriative and set it equal to zero: $$ f'(x) =  \sum\limits_{i = 1}^n 2(x_i - x) = 0$$ so $$\sum\limits_{i = 1}^n x_i - nx = 0$$ and $$x = \frac{x_1 +...+x_n}{n}$$. Since $f$ is just a second degree polynomial with positive leading term this x will be a global minima - so we've proved our claim - the mean minimizes the MSE. Now since the square root is a monotonic function the mean will also mimize the Root Mean Square Error and is also the _unique_ value that does so.

Ok great but the metric for our competition is not RMSE nor is it RMAE but RMSLE. What does this mean? Well let's look at the definition of RMSLE for a given array $[x_1,..., x_n]$ and a variable $a$: $$RMSLE(x) =   \sqrt{(\sum\limits_{i = 1}^n (log(x_i + 1) - log(x + 1))^2)}$$ 
This is very similar to RMSE! In fact one can easily check that the following holds: $$RSMLE(x) = RMSE(log(x) + 1)$$

So remember we're trying to find the value the minimizes the RMSLE, putting everything we've figured out togther we have:

**Lemma 2**: The unique value that minimizes the RMSLE is $$exp(log(mean(X) + 1)) - 1$$

Proof: We're trying to minimize $$RMSLE(x) = RMSE(log(x) + 1)$$ so by lemma 1 we get that $log(x) + 1 = mean(X)$ and solving for $x$ we get that $x = exp(log(mean(X) + 1)) - 1$ and we're done.

Note that in R you can write this is `exp1m(log1p(mean(X)))` 

Ok enough theory, let's see if this actully hold on our dataset:

```{r, message=FALSE}
library(data.table)
library(Metrics)

n = 15e6

train = fread("/Users/alexpapiu/Documents/Data/Bimbo/train.csv", 
              select = c("Semana", "Cliente_ID", "Producto_ID", "Demanda_uni_equil"),
              nrow = n)

val = train[train$Semana == 4]
train = train[train$Semana == 3]
```

```{r}
head(train)
```

Remember we're trying to predict `Demanda_uni_equil`. Let's just predict using 
```{r}
y = train$Demanda_uni_equil
mean_demand = mean(y)
median_demand = median(y)
log1p_mean = expm1(mean(log1p(y)))
```


```{r}
results = c(rmsle(y, mean_demand), rmsle(y, median_demand), rmsle(y, log1p_mean))
names(results) = c("mean", "median", "log1p_mean")
barplot(results)
```
And indeed the results confirm the math (they better): the mean of the log transformed demand gets the best rmsle beating both the mean and median. The median also does much better than the mean - this is because of the demand distribution being asymetric. Note however the `expm1(mean(log1p(y)))` will _always_ get a better rsmle regardless of the distribution of y.

Let's also do this on the validation set. Here the math doesn't guarantee anything:

```{r}
y_val = val$Demanda_uni_equil
results = c(rmsle(y_val, mean_demand), rmsle(y_val, median_demand), rmsle(y_val, log1p_mean))
names(results) = c("mean", "median", "log1p_mean")
barplot(results)
```
 We get similar results on the validation set.
 
 ###Averaging by Group:
 However a global average can only get you so far. Another thing you to try is to group by some categorical variable and average for each group. There are a few scripts doing this already - especially R_medians. 
 
 
 
```{r}
train[, .(mean = mean(Demanda_uni_equil),
          median = median(as.double(Demanda_uni_equil)), #data.table doesn't like ints for medians
          log1p_mean = expm1(mean(log1p(Demanda_uni_equil)))), by = .(Producto_ID)] %>% 
    merge(val, all.y = TRUE, by = "Producto_ID") -> merged_val #merge with validation set by product

head(merged_val)
```

Now let's compute the rmsle and visualize the results again:
```{r}
y_val = merged_val$Demanda_uni_equil
results = c(rmsle(y_val, merged_val$mean), 
            rmsle(y_val, merged_val$median),
            rmsle(y_val, merged_val$log1p_mean))
names(results) = c("mean", "median", "log1p_mean")
barplot(results)
```

